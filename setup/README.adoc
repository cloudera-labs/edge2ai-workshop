= Single Node EDGE2AI CDH Cluster

This script automatically sets up a CDH cluster on the public cloud on a single VM with the following 16 services: 

[%autowidth,cols="1a,1a,1a,1a,1a",options="header"]
|====
^|CEM ^|CFM ^|CSP ^|CDH ^|CDSW
|* MiNiFi
* EFM
|* NiFi
* NiFi Registry
* NiFi CA Service`
|* Kafka
* Schema Registry
* Streams Messaging Manager
|* ZooKeeper
* HDFS
* YARN
* Spark
* Hive
* Impala
* Kudu
* Hue
* Oozie
|* CDSW
|====

This cluster is meant to be used for demos, experimenting, training, and workshops.

== Namespaces

The automated setup scripts support the concept of *namespaces*. All the resources of a single deployment are associated with a single namespace. By creating multiple namespaces, one can manage multiple independent deployments without them interfering with each other.

A namespace is defined by simply creating a configuration file called `.env.<namespace>`, where `<namespace>` is a string that identifies the namespace uniquely. The namespace identifier cannot contain spaces or special characters.

[[stopping-instances]]
== Stopping and starting instances

IMPORTANT: Please read this section before stopping instances to ensure you can start them again successfully.

After the workshop environment is deployed it may be desirable to stop instances during periods when they are not being used to reduce costs. This feature is *not enabled by default* but it can be turned on for certain deployments.

If you don't enable it correctly, as explained in this section, the clusters will *not* work correctly after a stop/start. So please read on!

==== Requirements

You *must* ensure that the following point are configured correctly before launching your environment:

* Look for the `TF_VAR_use_elastic_ip` in your `.env*` file and set it to `true`. If it doesn't existing in your `.env*` file, ensure the line below is added to it:
+
[source]
----
export TF_VAR_use_elastic_ip=true
----
* Once the property above it set, you must ensure that your AWS environment can allocate as many Elastic IPs as the number of VMs you're launching (# of cluster + 1 for the web server). The AWS default limit for Elastic IPs is very low (5 only), so make sure you check this and request an increase if it's too low for your needs.
** You can check the limit and request the increase in the AWS Console, under *EC2 > Limits > Search for "EC2-VPC Elastic IPs"*.
** If the limit is too low, select the *EC2-VPC Elastic IPs* line, click on *Request limit increase* and follow the prompts.
** The increase can be effective in a few minutes but sometimes it can take longer.

==== Before stopping your instances

If you followed the instructions above all your instances can be stopped after the deployment is complete. To ensure that the workshop services will continue to work correctly after instances are resumed, we recommend that you only stop the instances after the CDSW model deployment is completed.

Before you stop the instances, use the `check-service.sh` command to check the service status and confirm that all services are `Ok` and that the model has status `deployed`, as shown in the example below:

[source]
----
$ ./setup/terraform/check-services.sh default
instance                ip address  WEB   CM    CEM   NIFI  NREG  SREG  SMM   HUE   CDSW  Model Status
aws_instance.web[null]  1.2.3.4     Ok
aws_instance.cluster[0] 1.2.3.5           Ok    Ok    Ok    Ok    Ok    Ok    Ok    Ok    deployed
aws_instance.cluster[1] 1.2.3.6           Ok    Ok    Ok    Ok    Ok    Ok    Ok    Ok    deployed
----

The output `list-detail.sh` command contains a `Stoppable` column, which indicates if the instance can be safely stoppable or not.

[source]
----
$ ./setup/terraform/list-details.sh default
...
WEB SERVER VM:
==============
Web Server Name           Public DNS Name              Public IP  Private IP  Stoppable
araujo-default-web        ec2-1-2-3-4.compute.aws.com  1.2.3.4    10.0.0.1    Yes

CLUSTER VMS:
============
Cluster Name              Public DNS Name              Public IP  Private IP  Stoppable
araujo-default-cluster-0  ec2-1-2-3-5.compute.aws.com  1.2.3.5    10.0.0.2    Yes
araujo-default-cluster-1  ec2-1-2-3-6.compute.aws.com  1.2.3.6    10.0.0.3    Yes
----

==== Starting instances

After you start the instances it takes some time for the services, especially CDSW, to be ready to be used.

Monitor the services status with the `check-service.sh` command described above. If any of the services doesn't come up properly after several minutes, restart it manually through Cloudera Manager.

==== Known issues

The following services can show an unhealthy state after the stop/start. The are not normally used in the workshops, so this doesn't pose any major problem. If needed, they can be manually restarted from Cloudera Manager:

* HBase
* Livy
* Spark
* YARN

== User registration

For security reasons, the workshop environment is created by default without access from the public Internet. The environment includes a Web Server that allows users to register using a registration code. Once they register in the portal the admin is able to open the necessary firewall ports so that they can access their individual clusters.

The admin can control the registration process, opening and closing registration as needed, as explained below.

After launching your environment successfully (see the Setup section below), use the following steps to manage the registration process:

. Before the users can connect to the Web Portal to register it is necessary to allow limited access to the portal from the public Internet. To do this, run the following command:
+
[source,shell]
----
cd edge2ai-workshop/setup/terraform/
./open-registration.sh <namespace>
----
+
This command will allow public access to the Web Portal so that users can connect and start to register. The command will then enter a loop, monitoring the users' registration and will open the needed firewall ports for each individual user's IP address. Leave the command running while users are registering.

. Provide the users with the Web Server address and the registration code displayed on the screen by the command above so that they can start to register.

. Once you confirm that all users have registered, press `ENTER` on the window running the command above. The command will exit the monitoring loop, close the public access to the Web Server and ensure that all the necessary rules in place so that all users can continue to use the environment even without the public access rule.

=== Additional commands

The `open-registration.sh` command above is typically all you need to manage the registration process. The commands below, however, may come handy in certain situations:

* `sync-ip-addresses.sh` - this is the command that runs during the `open-registration.sh` loop to synchronize the user IPs registered in the portal with the environment security groups. If the IPs in the portal get out of sync with the IP rules in the environment's security groups, you can run the following command to sync them:
+
[source,shell]
----
./sync-ip-addresses.sh <namespace>
----

* `close-registration.sh` - if the `open-registration.sh` command didn't finish gracefully for any reason (e.g. window was closed or an error was thrown), the public access to the Web Portal will remain open. To close the public access (and the registration), run:
+
[source,shell]
----
./close-registration.sh <namespace>
----

* `update-registration-code.sh` - used to change the portal registration code if needed:
+
[source,shell]
----
./update-registration-code.sh <namespace>
----

* `manage-ip.sh` - used to manually add or remove IPs to/from the environment security groups:
+
[source,shell]
----
./manage-ip.sh <namespace> add <ip_address>
./manage-ip.sh <namespace> remove <ip_address>
----

== Setup

The setup of the workshop environment is fully automated. Before launching the workshop environment you need to ensure your laptop has the necessary pre-requisites to execute the setup script.

=== Pre-requisites

. Ensure a recent version of link:https://hub.docker.com/editions/community/docker-ce-desktop-mac[Docker] is running on your laptop
. Pull the latest Docker image for the workshop:
+
[source,shell]
----
docker pull asdaraujo/edge2ai-workshop:latest
----

=== Launching the workshop environment

. Clone this repository
+
[source,shell]
----
# Install git, skip if you already have it
sudo yum install -y git

# Clone the repo
git clone https://github.com/cloudera-labs/edge2ai-workshop.git
----

. Review software versions and edit them as needed. All the versions and locations of the software used for the setup are defined in the `edge2ai-workshop/setup/scripts/stack.sh` file. If this file does not exist, create it with one of the commands below:
+
[source,shell]
----
# For a CDH cluster:
cp edge2ai-workshop/setup/scripts/stack.template-cdh.sh edge2ai-workshop/setup/scripts/stack.sh

# For a CDP-DC cluster:
cp edge2ai-workshop/setup/scripts/stack.template-cdp.sh edge2ai-workshop/setup/scripts/stack.sh
----
+
The `stack.sh` file is the default stack definition file used by all the namespaces. You can create namespace-specific definitions by creating the following file for a given namespace:
+
[source,shell]
----
# For a CDH cluster:
cp edge2ai-workshop/setup/scripts/stack.template-cdh.sh edge2ai-workshop/setup/scripts/stack.<namespace>.sh

# For a CDP-DC cluster:
cp edge2ai-workshop/setup/scripts/stack.template-cdp.sh edge2ai-workshop/setup/scripts/stack.<namespace>.sh
----
+
IMPORTANT: Most of the software locations are already pre-defined in the template. For a few of the required software, though, there's currently no public repository available. Make sure you follow the instructions below to install these components.
+
For Schema Registry (SR) and Streams Messaging Manager (SMM) installation you have two options:

* If you have an URL link to a location where the CSP parcel and SR/SMM CSDs can be downloaded from, configure the corresponding properties in the `stack.sh` (or `stack.<namespace>.sh`) file:
+
[source,shell]
----
SCHEMAREGISTRY_VERSION=
STREAMS_MESSAGING_MANAGER_VERSION=
CSP_PARCEL_REPO=
SCHEMAREGISTRY_CSD_URL=
STREAMS_MESSAGING_MANAGER_CSD_URL=
----
+
NOTE: The version of the components, as indicated in the `SCHEMAREGISTRY_VERSION` and `STREAMS_MESSAGING_MANAGER_VERSION` variables, have the form: `<csd_version>.<parcel_version>`. For example, for the CSD binary `SCHEMAREGISTRY-0.8.0.jar` and parcel version `2.0.0.0-112` the Schema Registry version is `0.8.0.2.0.0.0-112`.

* Alternatively, leave the properties above unset and download the CSP parcel into `edge2ai-workshop/setup/parcels/` and both CSDs into `edge2ai-workshop/setup/csds/`. Note that the setup process will upload these files to every VM it creates, so if you're launching a large nuber of VMs, expect a good amount of upload volume.

. Review the cloud and workshop environment definitions and edit them as needed. This information is defined in the
+
[source,shell]
----
cp edge2ai-workshop/setup/terraform/.env.template edge2ai-workshop/setup/terraform/.env.<namespace>
chmod 400 edge2ai-workshop/setup/terraform/.env.<namespace>
----
+
where `<namespace>` is an arbitrary name for your environment namespace.
+
The variables in this file are explained below:
+
--
* `TF_VAR_cluster_count`: number of one-node cluster to be created
* `TF_VAR_registration_code`: registration code for the workshop environment. This is the code to be used as a password for users to successfully register in the workshop Web Portal. If not set or empty, a random registration code will be generated when the environment is launched.
* `TF_VAR_use_elastic_ip`: enable elastic IPs for the environment VMs. This is required if you need to stop/start VMs.

'''

* `TF_VAR_owner`: your user id. This will be used to tag your cloud resources.
* `TF_VAR_web_server_admin_email`: email used by the Web Server admin. This will only be used to identify the admin upon logging in to the Web Server.
* `TF_VAR_web_server_admin_password`: Web Server admin password.

'''

* `TF_VAR_aws_region`: AWS region to use
* `TF_VAR_aws_access_key_id`: Your AWS Access Key Id
* `TF_VAR_aws_secret_access_key`: Your AWS Secret Access Key

'''

* `TF_VAR_deploy_cdsw_model`: Whether or not to deploy the CDSW model. If set to `false` CDSW will be installed but the workshop model will *not* be deployment. Default is `true`, which causes the model to be deployed.

'''

* `TF_VAR_cluster_ami`: AMI ID to use for the one-node cluster. Ensure you pick a Centos 7 image.
+
NOTE: This is a vanilla Centos 7 AMI. No other prerequistes are necessary. All the required software will be installed by the setup process.
* `TF_VAR_ssh_username`: The username used to log in to the VM. Typically: `centos`
* `TF_VAR_cluster_instance_type`: Instance type to use for the one-node cluster. Recommended: `m5.4xlarge` or later/larger.

'''

* `TF_VAR_project`: Project name. This is used for instance tagging.
* `TF_VAR_enddate`: End date in MMDDYYYY format. This is used for instance tagging. Some Cloudera environment use this to automatically kill "expired" instances.
--

. Launch your environment
+
[source,shell]
----
cd edge2ai-workshop/setup/terraform/
./launch.sh <namespace>
----
+
where `<namespace>` is the name of one of your namespaces.

+
At the end of the script execution it will list the following information for all the clusters. This information should be provided to the workshop attendees:

* Public DNS Name
* Public IP
* Private DNS Name

A private key file will also be created on the local directory for authenticating the connections to the clusters.

. Once the workshop is completed, terminate all the environments with the following command:
+
[source,shell]
----
# cd edge2ai-workshop/setup/terraform/
./terminate.sh <namespace>
----
+
where `<namespace>` is the name of one of your namespaces.

. A few helper scripts are provided to help connecting to the clusters:

* `./list-details.sh [namespace]` - if run without arguments it will display a summary of all the existing environments. If a namespace is specified, it will display the details for all the clusters on that environment (public DNS, public IP and private DNS).
* `./check-services.sh <namespace>` - perform a health check of all the cluster to verify if all the services are up and running.
* `./connect-to-cluster.sh <namespace> <cluster_number>` - connect to the specified cluster using SSH.
* `./browse-cluster.sh <namespace> <cluster_number>` - (MacOS only) Opens a Chrome browser with all the tabs required for the workshop. All the URLs use the cluster's public DNS name.
* `./browse-cluster-socks.sh <namespace> <cluster_number>` - (MacOS only) Same as above, but using URLs with the private DNS name, instead, and setting the browser to use a SOCKS proxy, which is spawn by the script.
* `./run-on-cluster.sh <namespace> <cluster_number> '<command>'` - run a command on the specified cluster.
* `./run-on-all-clusters.sh <namespace> '<command>'` - run a command on all clusters.
* `./upload-instance-details.sh <namespace> [web_ip_adress] [admin_email] [admin_password] [admin_full_name]` - upload all the instances' details to the web server. If no parameters are specified it will use the default web server for the current deployment, otherwise will upload to the specified webserver. Note that this script is automatically executed upon launch for the current web server.

Clusters numbers start from 0 (zero).

== Use

* Once the script returns, you can open Cloudera Manager at http://<public_dns>:7180. The default credentials are `admin/admin`.

* Wait for about 10-20 mins for CDSW to be ready. You can monitor the status of CDSW by issuing the `cdsw status` command.

* You can use `kubectl get pods -n kube-system` to check if all the pods that the role `Master` is suppose to start have really started.

* You can also check the CDSW deployment status on `CM > CDSW service > Instances > Master role > Processes > stdout`.

== Other setup scripts

=== SMM Truck Demo
The scripts necessary to run the SMM Truck Demo are deployed to the cluster instances upon launch but are *not* executed.

To complete the setup for the SMM Truck Demo, follow the steps below:

==== Run for all clusters.

* Go to the terminal window where you launched the workshop (under the `setup/terraform` directory)
* Run the SMM Truck Demo setup (only needed once):
+
[source,shell]
----
./run-on-all-clusters.sh <namespace> "sudo /opt/dataloader/smm-generator.sh setup"
----
+
--
The setup will:

* Deploy all the necessary scripts and files
* Load the NiFi flows used in the demo
* Start all the NiFi Controller Services
--

After the setup is complete, you can start/stop the consumer and producers as many times as needed:

* To start all the consumers and producers:
+
[source,shell]
----
./run-on-all-clusters.sh <namespace> "sudo /opt/dataloader/smm-generator.sh start"
----

* To stop all the consumers and producers:
+
[source,shell]
----
./run-on-all-clusters.sh <namespace> "sudo /opt/dataloader/smm-generator.sh stop"
----

* You can check the producer and consumer status using:
+
[source,shell]
----
./run-on-all-clusters.sh <namespace> "sudo /opt/dataloader/smm-generator.sh status"
----
+
--
After a successful start there should be 43 clients running: 13 consumers and 30 producers. There are 2 different types of consumers and 2 of producers. The status command shows the number of running consumers and producers by type, as shown below:

[source,python]
----
      3 LoggerAvroEventConsumer
     10 LoggerStringEventConsumer
      9 SMMSimulationRunnerSingleDriverApp
     21 SMMSimulationRunnerTruckFleetApp
----

--

==== Run for a single node

You can also run the commands above for a single instance.

1. SSH to the cluster instance
2. Run the commands as per below:

* *Setup*: `sudo /opt/dataloader/smm-generator.sh setup`
* *Start*: `sudo /opt/dataloader/smm-generator.sh start`
* *Stop*: `sudo /opt/dataloader/smm-generator.sh stop`
* *Status*: `sudo /opt/dataloader/smm-generator.sh status`

== Running without Docker

If you need or want to run the setup scripts without installing Docker, you should install the following prerequisites in your laptop.

. Install Terraform
+
This setup uses link:https://www.terraform.io/[Terraform] to spin up the VMs and execute the required setup scripts.

.. Check if Terraform is installed and version is 0.12.3 or later
+
[source,shell]
----
terraform version
----
.. If Terraform is not installed or the version is lower, install a later version:
+
[source,shell]
----
# The URL below is for Linux. For Terraform on Mac see www.terraform.io/downloads.html
curl -O https://releases.hashicorp.com/terraform/0.12.6/terraform_0.12.6_linux_amd64.zip
mkdir ./bin
unzip -d ./bin/ terraform_0.12.6_linux_amd64.zip
export PATH=$PWD/bin:$PATH
----

. Install `jq`
.. If you are using a Mac, you can install `jq` using Homebrew:
+
[source,shell]
----
brew update
brew install jq
----

.. Otherwise, see download and install instructions link:https://stedolan.github.io/jq/download/[here]. Make sure `jq` is in your PATH after it's installed.

. Install required Python modules
+
[source,shell]
----
pip install jinja2 pyyaml awscli
----

== Troubleshooting and known issues

=== Clock Offset

The NTPD service which is required by Kudu and the Host is not installed. For the moment, just put
`--use-hybrid-clock=false`  in Kudu's Configuration property `Kudu Service Advanced Configuration Snippet (Safety Valve) for gflagfile` and suppressed all other warnings.

=== Docker device

To find out what the docker device mount point is, use `lsblk`. See below examples:

See examples below:

==== AWS, using a M5.2xlarge or M5.4xlarge VM:

[source,shell]
----
$ lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme0n1     259:1    0  100G  0 disk
+-nvme0n1p1 259:2    0  100G  0 part /
nvme1n1     259:0    0 1000G  0 disk

$ ./setup.sh aws cluster_template.json /dev/nvme1n1
----

==== Azure Standard D8s v3 or Standard D16s v3

[source,shell]
----
$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
fd0      2:0    1    4K  0 disk
sda      8:0    0   30G  0 disk
+-sda1   8:1    0  500M  0 part /boot
+-sda2   8:2    0 29.5G  0 part /
sdb      8:16   0   56G  0 disk
+-sdb1   8:17   0   56G  0 part /mnt/resource
sdc      8:32   0 1000G  0 disk
sr0     11:0    1  628K  0 rom

$ ./setup.sh azure cluster_template.json /dev/sdc
----

==== GCP n1-standard-8 or n1-standard-16

[source,shell]
----
$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0  100G  0 disk 
└─sda1   8:1    0  100G  0 part /
sdb      8:16   0 1000G  0 disk 

$ ./setup.sh gcp cluster_template.json /dev/sdb
----
